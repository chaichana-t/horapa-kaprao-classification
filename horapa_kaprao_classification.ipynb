{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "horapa-kaprao-classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TAUTOLOGY-EDUCATION/DATASET.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBwlBIzeRCnE",
        "outputId": "594115ac-9774-41ce-bf82-28c03c45017e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DATASET'...\n",
            "remote: Enumerating objects: 561, done.\u001b[K\n",
            "remote: Counting objects: 100% (102/102), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 561 (delta 5), reused 100 (delta 4), pack-reused 459\u001b[K\n",
            "Receiving objects: 100% (561/561), 1.93 GiB | 43.31 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "Checking out files: 100% (540/540), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcME9XuSRwQV",
        "outputId": "f614afc6-0b44-4489-b01e-18f1cb28ca69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 35.8 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 24.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YSCKDy9kQ9FW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import albumentations\n",
        "import albumentations.pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_state=32\n",
        "sns.set_theme(color_codes=True)"
      ],
      "metadata": {
        "id": "YG5D5Q0NWIFi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "Y3FESMaLiv2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "\n",
        "    data_dir = './DATASET/HorapaVsKaprao'\n",
        "\n",
        "    classes = ['horapa', 'kaprao']\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    mean_imagenet = (0.485, 0.456, 0.406)\n",
        "    std_imagenet  = (0.229, 0.224, 0.225)\n",
        "\n",
        "    model_name = 'tf_efficientnetv2_s'\n",
        "\n",
        "    if model_name == 'tf_efficientnetv2_s':\n",
        "        image_size = 300\n",
        "    elif model_name in ['tf_efficientnetv2_m', 'tf_efficientnetv2_l']:\n",
        "        # image_size = 384\n",
        "        image_size = 224\n",
        "    elif model_name == 'tf_efficientnetv2_b3':\n",
        "        image_size = 240\n",
        "    elif model_name in ['convnext_large_in22k', 'convnext_xlarge_in22k']:\n",
        "        image_size = 224\n",
        "    elif model_name == 'vit_base_patch16_224':\n",
        "        image_size = 224\n",
        "\n",
        "    batch_size = 16\n",
        "\n",
        "    lr = 0.00005\n",
        "    gamma = 0.97"
      ],
      "metadata": {
        "id": "37LpTkXhVJf0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "WYwPYgoMTZSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HorapaKapraoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, labels, image_size=224, dataset='train', transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.image_size = image_size\n",
        "        self.dataset = dataset\n",
        "\n",
        "        if transform == None:\n",
        "            self.transform = self.get_transform()\n",
        "        else:\n",
        "            self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        image = self.load_image(image_path=image_path)\n",
        "        image = self.transform(image=image)['image']\n",
        "        label = self.labels[index]\n",
        "        return image, label\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        image = np.array(Image.open(image_path).convert('RGB'))\n",
        "        return image\n",
        "    \n",
        "    def get_transform(self):\n",
        "        if self.dataset == 'train':\n",
        "            transform = albumentations.Compose([\n",
        "                albumentations.OneOf([\n",
        "                    albumentations.HorizontalFlip(p=1),\n",
        "                    albumentations.VerticalFlip(p=1),\n",
        "                    albumentations.Rotate(limit=360, p=1),                  \n",
        "                ], p=0.5),\n",
        "                albumentations.RandomCrop(height=self.image_size, width=self.image_size, p=0.5),\n",
        "                albumentations.ShiftScaleRotate(border_mode=cv2.BORDER_CONSTANT, scale_limit=0.3,\n",
        "                                                rotate_limit=(10, 30), p=0.5),\n",
        "                albumentations.MotionBlur(p=0.5),\n",
        "                albumentations.OpticalDistortion(p=0.5),\n",
        "                albumentations.GaussNoise(p=0.5),\n",
        "                albumentations.Resize(width=self.image_size, height=self.image_size,\n",
        "                                      interpolation=cv2.INTER_CUBIC),\n",
        "                albumentations.Normalize(mean=config.mean_imagenet, std=config.std_imagenet),\n",
        "                albumentations.pytorch.ToTensor(),             \n",
        "            ])\n",
        "        else:\n",
        "            transform = albumentations.Compose([\n",
        "                albumentations.Resize(width=self.image_size, height=self.image_size,\n",
        "                                      interpolation=cv2.INTER_CUBIC),\n",
        "                albumentations.Normalize(mean=config.mean_imagenet, std=config.std_imagenet),\n",
        "                albumentations.pytorch.ToTensor(),                               \n",
        "            ])\n",
        "        \n",
        "        return transform\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    data = []\n",
        "    for cls in os.listdir(config.data_dir):\n",
        "        image_dir = os.path.join(config.data_dir, cls)\n",
        "        for image_name in os.listdir(image_dir):\n",
        "            data.append({\n",
        "                'image_path': os.path.join(image_dir, image_name),\n",
        "                'label': config.classes.index(cls)\n",
        "            })\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_dataloader():\n",
        "    df = get_data()\n",
        "    image_paths, labels = df['image_path'].tolist(), df['label'].tolist()\n",
        "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "        image_paths, labels, test_size=0.2, stratify=labels, random_state=random_state\n",
        "    )\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        train_paths, train_labels, test_size=0.2, stratify=train_labels, random_state=random_state\n",
        "    )\n",
        "\n",
        "    train_dataset = HorapaKapraoDataset(image_paths=train_paths, labels=train_labels,\n",
        "                                        image_size=config.image_size, dataset='train',\n",
        "                                        transform=None)\n",
        "    val_dataset   = HorapaKapraoDataset(image_paths=val_paths, labels=val_labels,\n",
        "                                        image_size=config.image_size, dataset='val',\n",
        "                                        transform=None)\n",
        "    test_dataset  = HorapaKapraoDataset(image_paths=test_paths, labels=test_labels,\n",
        "                                        image_size=config.image_size, dataset='test',\n",
        "                                        transform=None)\n",
        "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=config.batch_size,\n",
        "                                  shuffle=True, pin_memory=True)\n",
        "    val_dataloader   = DataLoader(dataset=val_dataset, batch_size=config.batch_size,\n",
        "                                  shuffle=False, pin_memory=True)\n",
        "    test_dataloader  = DataLoader(dataset=test_dataset, batch_size=1,\n",
        "                                  shuffle=False, pin_memory=True)\n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "uw_uQHmUTH6F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "pUbTiLT-iy4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.cnn = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "        if model_name in ['tf_efficientnetv2_s', 'tf_efficientnetv2_m', 'tf_efficientnetv2_l',\n",
        "                          'tf_efficientnetv2_xl_in21k']:\n",
        "            in_features = 1280\n",
        "        elif model_name in ['tf_efficientnetv2_b3']:\n",
        "            in_features = 1536\n",
        "        elif 'convnext' in model_name:\n",
        "            in_features = 2048\n",
        "        elif model_name in ['vit_base_patch16_224']:\n",
        "            in_features = 768\n",
        "        \n",
        "        if 'convnext' in model_name:\n",
        "            self.cnn.head.fc = nn.Linear(in_features=in_features, out_features=config.num_classes)\n",
        "        elif 'tf_efficientnetv2' in model_name:\n",
        "            self.cnn.classifier = nn.Linear(in_features=in_features, out_features=config.num_classes)\n",
        "        elif 'vit' in model_name:\n",
        "            self.head = nn.Linear(in_features=in_features, out_features=config.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "    \n",
        "    def freeze(self):\n",
        "        for name, parameter in self.named_parameters():\n",
        "            if not 'classifier' in name:\n",
        "                parameter.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for parameter in self.parameters():\n",
        "            parameter.requires_grad = True"
      ],
      "metadata": {
        "id": "fbLAtL15U35N"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "jFerybI1i0Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_true):\n",
        "    y_pred = nn.functional.softmax(y_pred, dim=1)\n",
        "    _, top_class = y_pred.topk(1, dim=1)\n",
        "    equals = top_class == y_true.view(*top_class.shape)\n",
        "    return torch.sum(equals.type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, criterion, optimizer, scheduler=None):\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f'Device: {self.device}')\n",
        "\n",
        "    def train_batch_loop(self, model, dataloader):\n",
        "        train_loss = 0.0\n",
        "        train_acc  = 0.0\n",
        "\n",
        "        for inputs, labels in tqdm(dataloader):\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_acc += accuracy(logits, labels)\n",
        "        \n",
        "        return train_loss / len(dataloader), train_acc / len(dataloader.dataset)\n",
        "\n",
        "    def valid_batch_loop(self, model, dataloader):\n",
        "        val_loss = 0.0\n",
        "        val_acc  = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(dataloader):\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                logits = model(inputs)\n",
        "\n",
        "                loss = self.criterion(logits, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_acc += accuracy(logits, labels)\n",
        "          \n",
        "        return val_loss / len(dataloader), val_acc / len(dataloader.dataset)\n",
        "\n",
        "    def train(self, model, train_dataloader, val_dataloader, epochs, save_weights_path='model.pt'):\n",
        "        model = nn.DataParallel(model)\n",
        "        model.to(self.device)\n",
        "\n",
        "        val_min_loss = np.Inf\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_loss, train_acc = self.train_batch_loop(model=model, dataloader=train_dataloader)\n",
        "\n",
        "            model.eval()\n",
        "            val_loss, val_acc = self.valid_batch_loop(model=model, dataloader=val_dataloader)\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss <= val_min_loss:\n",
        "                print('val_loss improved, saving model weight to {}'.format(save_weights_path))\n",
        "                torch.save(model.module.state_dict(), save_weights_path)\n",
        "                val_min_loss = val_loss\n",
        "\n",
        "            print('Epoch : {}'.format(epoch+1))\n",
        "            print('   train_loss : {:.6f} train_acc : {:.6f}'.format(train_loss, train_acc))\n",
        "            print('   val_loss   : {:.6f} val_acc   : {:.6f}'.format(val_loss, val_acc))"
      ],
      "metadata": {
        "id": "uXZXQI2kXLPF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, val_dataloader, test_dataloader = get_dataloader()\n",
        "\n",
        "model = Model(model_name=config.model_name)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=config.lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=config.gamma)\n",
        "\n",
        "trainer = Trainer(criterion=criterion, optimizer=optimizer, scheduler=scheduler)\n",
        "trainer.train(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, \n",
        "              epochs=20, save_weights_path=f'{config.model_name}.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jVdqhumhXO2W",
        "outputId": "915bdeb2-f953-48b2-c177-b06b1640bec9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [04:47<00:00, 13.05s/it]\n",
            "100%|██████████| 6/6 [00:20<00:00,  3.46s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 1\n",
            "   train_loss : 0.670756 train_acc : 0.561404\n",
            "   val_loss   : 0.457914 val_acc   : 0.941860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [04:41<00:00, 12.82s/it]\n",
            "100%|██████████| 6/6 [00:20<00:00,  3.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 2\n",
            "   train_loss : 0.532995 train_acc : 0.757310\n",
            "   val_loss   : 0.292707 val_acc   : 0.988372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [03:55<00:00, 10.69s/it]\n",
            "100%|██████████| 6/6 [00:20<00:00,  3.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 3\n",
            "   train_loss : 0.422750 train_acc : 0.868421\n",
            "   val_loss   : 0.245748 val_acc   : 0.976744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [04:26<00:00, 12.12s/it]\n",
            "100%|██████████| 6/6 [00:20<00:00,  3.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 4\n",
            "   train_loss : 0.344481 train_acc : 0.885965\n",
            "   val_loss   : 0.142840 val_acc   : 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [03:51<00:00, 10.51s/it]\n",
            "100%|██████████| 6/6 [00:20<00:00,  3.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 5\n",
            "   train_loss : 0.238186 train_acc : 0.923977\n",
            "   val_loss   : 0.087999 val_acc   : 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [04:16<00:00, 11.64s/it]\n",
            "100%|██████████| 6/6 [00:20<00:00,  3.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 6\n",
            "   train_loss : 0.227100 train_acc : 0.929825\n",
            "   val_loss   : 0.051217 val_acc   : 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [04:34<00:00, 12.49s/it]\n",
            "100%|██████████| 6/6 [00:22<00:00,  3.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 7\n",
            "   train_loss : 0.228031 train_acc : 0.918129\n",
            "   val_loss   : 0.049215 val_acc   : 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [04:55<00:00, 13.42s/it]\n",
            "100%|██████████| 6/6 [00:27<00:00,  4.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_loss improved, saving model weight to tf_efficientnetv2_s.pt\n",
            "Epoch : 8\n",
            "   train_loss : 0.231167 train_acc : 0.909357\n",
            "   val_loss   : 0.034278 val_acc   : 1.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/22 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2e8e2e8444de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m trainer.train(model=model, train_dataloader=train_dataloader, val_dataloader=val_dataloader, \n\u001b[0;32m---> 11\u001b[0;31m               epochs=20, save_weights_path=f'{config.model_name}.pt')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-bd64dcad85cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_dataloader, val_dataloader, epochs, save_weights_path)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-bd64dcad85cc>\u001b[0m in \u001b[0;36mtrain_batch_loop\u001b[0;34m(self, model, dataloader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-dfba66f0d990>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-dfba66f0d990>\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "qV9VUx6mi2m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(f'{config.model_name}.pt'))\n",
        "model.eval()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_dataloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        logits = model(inputs)\n",
        "\n",
        "        y_true.append(config.classes[labels.item()])\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        y_pred.append(config.classes[predicted.item()])\n",
        "\n",
        "print(classification_report(y_true, y_pred, digits=4))\n",
        "plt.figure(figsize=(7, 5))\n",
        "ax = sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, cmap='Blues')\n",
        "ax.set(xlabel='Predict', ylabel='Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "q7t8qVL2XUNu",
        "outputId": "0597ca87-21a2-4282-c8b0-e565a404fe14"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 107/107 [00:28<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      horapa     1.0000    0.9890    0.9945        91\n",
            "      kaprao     0.9412    1.0000    0.9697        16\n",
            "\n",
            "    accuracy                         0.9907       107\n",
            "   macro avg     0.9706    0.9945    0.9821       107\n",
            "weighted avg     0.9912    0.9907    0.9908       107\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V-ykb-I-hiiC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}